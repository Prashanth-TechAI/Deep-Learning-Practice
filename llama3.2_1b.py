# -*- coding: utf-8 -*-
"""llama3_21B_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SUECGgvCc2gGZa2Y4CFkNFVEKybc-aLu
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install gradio torch transformers

!pip install tiktoken

!pip install blobfile

from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaConfig

# Path to your LLaMA 3.2 1B model folder in Google Drive
model_path = '/content/drive/MyDrive/models/llama_folder/checkpoints/Llama3.2-1B/'

# Load the configuration file (ensure config.json exists)
config = LlamaConfig.from_pretrained(model_path)

# Load the tokenizer and model with the custom config
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, config=config, ignore_mismatched_sizes=True)

# Check if the model is loaded successfully
print("LLaMA 3.2 1B model loaded successfully with custom config!")

def chatbot(input_text):
    # Tokenize the input
    inputs = tokenizer(input_text, return_tensors="pt")

    # Generate output, limiting the length of the output (e.g., 100 tokens)
    outputs = model.generate(inputs["input_ids"], max_length=100, num_return_sequences=1)

    # Decode and return response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response

import gradio as gr

# Create Gradio interface
iface = gr.Interface(fn=chatbot, inputs="text", outputs="text", title="LLaMA 3.2 1B Chatbot", description="Chat with the LLaMA 3.2 1B model")

# Launch the interface
iface.launch(share=True)  # share=True to generate a public link